
<html>
<head>
<title> CS440/640 Homework 2: Pauline Ramirez </title>
<style>
<!--
body{
font-family: 'Trebuchet MS', Verdana;
}
p{
font-family: 'Trebuchet MS', Times;
margin: 10px 10px 15px 20px;
}
h3{
margin: 5px;
}
h2{
margin: 10px;
}
h1{
margin: 10px 0px 0px 20px;
}
div.main-body{
align:center;
margin: 30px;
}
hr{
margin:20px 0px 20px 0px;
}

table, th, td {
    border: 1px solid black;
}
-->
</style>
</head>

<body>
<center>
<a href="http://www.bu.edu"><img border="0" src="http://www.cs.bu.edu/fac/betke/images/bu-logo.gif"
width="119" height="120"></a>
</center>

<h1>Gesture Recognition</h1>
<p> 
 CS 440 P2 <br>
 Pauline Ramirez<br>
 Jackie Andrade<br>Joseph Cho<br>
    April 5, 2017
</p>

<div class="main-body">
<hr>
<h2> Problem Definition </h2>


<p> In this assignment we implemented a program that can detect three hand gestures. The first two are static, and can detect a thumbs up and someone making a heart with their hands. The last gesture that it can detect is a hand wave. For the static gestures, we used skin color detection and size of bounding boxes to classify each gesture. For the dynamic gesture, we also used skin detection and then object tracking to determine whether or not the user waved. 




</p>

<hr>
<h2> Method and Implementation </h2>

<p>

We first decided to use the OpenCV library with Python, since we were most comfortable with that language. Using the built in functions, we were able to implement a skin detect function (similar to the one given to us in the skeleton code), and then used to to create bounding boxes around skin-colored objects. With the bounding boxes, we then checked the aspect ratio since it would be different for a thumbs up vs. the heart gesture. <br><br>

Here is an example of the skin differencing function in action, where black pixels represent anything that's not skin colored: <br>
<img src="440_pics/skinDetection.png" height="400" width="600"> <br><br>


For the dynamic motion, we used skin differencing again to detect a hand, then created a circle over it that would follow the center of the palm. This would allow the program to recognize a moving object. Using this we were able to track the movements of said object and determine whether or not it would fit with the actual movements of a waving hand. <br><br>

For each gesture that's classified correctly by the program, the text describing that gesture will appear on the screen to let the user know. You can see this in the screenshots we provide.

</p>
<hr>
<h2> Experiments and Results </h2>
<p> We did 50 trials and were able to create a confusion matrix to determine accuracy of our program.  <br><br>

To test each gesture, we tried multiple variations of the gesture to see if our program would accurately classify it, as well as gestures that were similar in shape to test the false positive rate. For example, since the program is implemented with checking the aspect ratio of a bounding box, there were some cases where the algorithm incorrectly classified a closed fist as a thumbs up, since the aspect ratio of a bounding box around a fist was similar to a thumbs up. We provided screenshots below to show this.

<br> <br><br>

<b> Thumbs Up </b> <br>

Confusion Matrix based on 50 trials: <br>

<table>
	<tr>
		<td width="50"> TP: 21 </td>
		<td width="50"> FP: 10 </td>
	</tr>
		<td> FN: 7 </td>
		<td> TN: 12 </td>


</table> <br><br> Meaning that the accuracy for detecting a thumbs up gesture was about 66%, we will discuss below the factors that contributed to this accuracy rate. <br>

An example of a true positive: <br>
<img src="440_pics/thumbsup_TP.png" height="400" width="600"> <br><br>

An example of a false positive: <br>
<img src="440_pics/thumbsup_FP.png" height="400" width="600"> <br><br>

An example of a true negative: <br>
<img src="440_pics/thumbsup_TN.png" height="400" width="600"> <br><br>

An example of a false negative: <br>
<img src="440_pics/thumbsup_FN.png" height="400" width="600"> <br><br>



<br> <br><br>

<b> Heart </b> <br>

Confusion Matrix based on 50 trials: <br>

<table>
	<tr>
		<td width="50"> TP: 22 </td>
		<td width="50"> FP: 11 </td>
	</tr>
		<td> FN: 9 </td>
		<td> TN: 8 </td>

</table> <br><br> Meaning that the accuracy for detecting a heart gesture was around 60%. 

An example of a true positive: <br>
<img src="440_pics/heart_TP.png" height="400" width="600"> <br><br>

An example of a false positive: <br>
<img src="440_pics/heart_FP.png" height="400" width="600"> <br><br>

An example of a true negative: <br>
<img src="440_pics/heart_TN.png" height="400" width="600"> <br><br>

An example of a false negative: <br>
<img src="440_pics/heart_FN.png" height="400" width="600"> <br><br>


The way these were implemented was by using bounding boxes around the contours that were determined from the skin detection. We then checked the aspect ratio of these bounding boxes and used that to create functions that would verify whether the bounding box matched with a thumbs up gesture as opposed to a heart gesture. Here is a screenshot showing the bounding boxes in action: <br>

<img src="440_pics/BoundingBoxes.png" height="400" width="600"> <br><br>

<b> Hand Wave </b> <br>

Confusion Matrix based on 50 trials: <br>

<table>
	<tr>
		<td width="50"> TP: 14 </td>
		<td width="50"> FP: 14 </td>
	</tr>
		<td> FN: 8 </td>
		<td> TN: 14 </td>

</table> <br><br> Meaning that the accuracy for detecting a hand wave gesture was around 56%. 

An example of a true positive: <br>
<img src="440_pics/dynammic_TP.png" height="400" width="600"> <br><br>

An example of a false positive: <br>
<img src="440_pics/dynammic_FP.png" height="400" width="600"> <br><br>

An example of a true negative (Jackie was moving her hand up and down): <br>
<img src="440_pics/dynammic_TN.png" height="400" width="600"> <br><br>

An example of a false negative (Jackie was actually waving): <br>
<img src="440_pics/dynammic_FN.png" height="400" width="600"> <br><br>

The way these were implemented was by tracking an object using the skin detection function that was given to us. We found the centroid of the object and created a deque that contained all the points of the centroid over time. Using this deque, we were able to plot a trail of the object, and use the shape of that to detect a hand wave. To do this, we checked that the y-coordinates of the centroid remained relatively similar, while the x-coordinates would change over time to mimick a hand wave gesture. An example is shown below, where you can see the centroid of the hand and the trail that shows it's movement:<br>

<img src="440_pics/MotionDetections.png" height="400" width="600"> <br><br>

</p>

<hr>
<h2> Discussion </h2>

<p> It was clear from our experiments that our program was not 100% accurate, which is what we expected. One factor that caused this was that the skin differencing function would often classify our backgrounds as skin, which would cause a lot of problems in the rest of the program. For example, there were times where bounding boxes would be drawn on a window behind us or the wall. Another issue is that the aspect ratios used to classify each gesture are hard coded, meaning that if someone with larger/smaller hands were to try the program, it would be a little harder to classify their gestures. <br><br>

We found that the hand wave gesture recognition was the least accurate, and this was again due to the skin differencing function being sensitive to our background. There were cases where the object tracker would jump around the screen, causing issues with it being able to track a hand wave gesture. If we had more time to work on this program, we would have to make it less sensitive to backgrounds, and we believe that this could make the program more accurate. 



</p>

<hr>
<h2> Conclusions </h2>

<p>
Regardless of the accuracy issues, we're pretty satisfied with how the program came out. It was able to detect the two static motions (under careful circumstances), as well as being able to detect a hand waving. The only thing is that we wish we had more time to finish this assignment, since a massive obstacle was getting started with the OpenCV library. 
</p>


<hr>
<h2> Credits and Bibliography </h2>


<p>Sources used:

<ul>
	<li>http://www.pyimagesearch.com/2015/09/14/ball-tracking-with-opencv/</li>
	<li>http://www.pyimagesearch.com/2014/08/18/skin-detection-step-step-example-using-python-opencv/ </li>

</ul>

</p>


<hr>
</div>
</body>



</html>
